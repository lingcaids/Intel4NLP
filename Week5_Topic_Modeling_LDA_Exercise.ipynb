{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Using LDA to Understand Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "We'll be using some of the fetch20 dataset built into SkLearn to use LDA for topic modeling. Let's start by loading 3 categories from the dataset. I recommend choosing 3 fairly disparate categories to begin with. You can see the full list of available categories here: http://scikit-learn.org/stable/datasets/twenty_newsgroups.html. \n",
    "\n",
    "## Question 1\n",
    "\n",
    "* Load ONLY the training set from each of these categories \n",
    "* Remove the headers, footers, and quotes from each member of the set\n",
    "* Explore the dataset and verify that you have inputs from each category (are the datapoints randomized?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tSorry, I was, but I somehow have misplaced my diskette from the last \n",
      "couple of months or so. However, thanks to the efforts of Bobby, it is being \n",
      "replenished rather quickly!  \n",
      "\n",
      "\tHere is a recent favorite:\n",
      "\n",
      "\t--\n",
      "\n",
      "\n",
      "       \"Satan and the Angels do not have freewill.  \n",
      "        They do what god tells them to do. \"\n",
      "\n",
      "        S.N. Mozumder (snm6394@ultb.isc.rit.edu) \n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "       \"Satan and the Angels do not have freewill.  \n",
      "        They do what god tells them to do. \"\n",
      "++\n",
      " \n",
      "\n",
      "Why not use the PD C library for reading/writing TIFF files? It took me a\n",
      "good 20 minutes to start using them in your own app.\n",
      "\n",
      "Martin\n",
      "\n",
      "--\n",
      "---------------------------------------------------------------------------\n",
      "++\n",
      " \n",
      "Indeed, if the color teal on a team's uniforms is any indication of the\n",
      "future, the Marlins are in dire trouble! Refer to the San Jose Sharks for\n",
      "proof... But I have hope for the Marlins. I was a sometime member of the\n",
      "Rene Lachemann fan club at the Oakland Coliseum, and have a deep respect\n",
      "for the guy. He's a gem. And, of course, Walt Weiss gives that franchise\n",
      "class. But yeah... whoever designed those uniforms was guilty of a paucity\n",
      "of style and imagination. Ugghhh!\n"
     ]
    }
   ],
   "source": [
    "print(ng_train.data[2])\n",
    "print(\"++\\n\", ng_train.data[1504])\n",
    "print(\"++\\n\", ng_train.data[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "* Pre-process all words in your document, including removing stop words.\n",
    "* Remove words that show up in more than 60% of the documents/\n",
    "* Vectorize your documents using NGrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# student section here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "* Create an LDA model with 3 topics. You can do this with GenSim or SkLearn.\n",
    "* Print out the topics and the 20 words most associated with that topic. \n",
    "* Try using more or less topics, is there a sweet spot that allows us to separate out the three input classes?\n",
    "* Find a document that is clearly about baseball, does the model choose it as dominantly the topic?\n",
    "* Use pyLDAvis (pip install pyldavis) to create an interactive visualization of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8fb6f940e160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# student section ends here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 4\n",
    "n_iter = 10\n",
    "# student section here\n",
    "\n",
    "\n",
    "# student section ends here\n",
    "data[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(ng_train.data[0]) # 99% composed of topic 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", ix)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(lda,count_vectorizer.get_feature_names(),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "**Topic 3 is baseball stuff and our post on baseball is predicted to be 99% topic 3. Nice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "* Open a new dataset from `data/ap/ap_split.txt` (Source: http://www.cs.columbia.edu/~blei/lda-c/). This is a dataset of articles from the associated press with no pre-determined scheme of topics. \n",
    "* Split this raw file into a set of documents. There is a clear marker between each article.\n",
    "* Clean the text data and prepare for modeling (note that each document has some \\<XYZ\\> tags as well as extra spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/ap_split.txt','r') as f:\n",
    "    raw_text = f.read()\n",
    "docs = raw_text.split('---')\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# student section here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# student section ends here\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "* Do LDA modeling to find topics in this chain of articles. Try many different numbers of topics and processing techniques. You can use GenSim or Sklearn.\n",
    "* Note: In this case, there isn't a \"right\" answer, but some answers are better than others. Try to find an answer where you're getting clear topics that make sense and seem consistent. Assign labels to each topic, after investigating it by eye, if you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# student section here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# student section here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "display_topics(lda,count_vectorizer.get_feature_names(),20) # We have to look at the topics before hand and then add the labels afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tn = [\"Political Media\",None,\"Financials\",None,\"Nordstrom Scandal\",\"Oil\",\"Hurricanes\",\"North Korea\",\"NASA\",\"US Politics\",\"TV Networks\",\"Forest Fires\",\n",
    "      None,\"Agriculture/Drought\",\"Middle East\",\"US Political Campaigns\",\"Pollution\",\"Carribean\",\"Health/Medical\",\"Theatre/Arts\",\"Global Warming\",\n",
    "      \"Advertisements\",\"Southern US Weather\",\"South America\",None]\n",
    "display_topics(lda,count_vectorizer.get_feature_names(),20,topic_names=tn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
